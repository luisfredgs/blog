I" <p class="image"><img src="/blog/assets/img/clustering-analysis-an-introduction.png" alt="Clustring Analysis - An introduction to unsupervised learning" /></p>

<p class="intro"><em>Cluster analysis is the grouping of individuals in a population in order to discover structure
in the data. In some sense, we would like the individuals within a group to be close
or similar to one another, but dissimilar from individuals in other groups.</em></p>

<p>In contrast to supervised learning (i.e classification), unsupervised learning fits a model to observations assuming there is no dependent random variable, output, or response. None of the observations is treated differently from the others. An informal way to say this is that there is no $Y$. For this reason, sometimes classification data that includes the $Y$ as the class is called labeled data but clustering data is called unlabeled.</p>

<p>Clustering is fundamentally a collection of methods of data exploration. One often uses a method to see if natural groupings are present in the data. If groupings do emerge, these may be named and their properties summarized. The results of a cluster analysis may produce an identifiable structure that can be used to generate hypotheses (to be tested on a separate data set) to account for the observed data.</p>

<h1 id="reasons-for-interest-in-unsupervised-procedures">Reasons for interest in unsupervised procedures</h1>

<p>Collecting and labeling a large set of sample patterns can be surprisingly costly. Possible solutions could be:</p>

<ul>
  <li>Train a classifier on a small set of labeled samples, and then ‚Äútuned up‚Äù by allowing it to run without supervision;</li>
  <li>Train a clustering algorithm with large amounts of unlabeled data, and only then use supervision to label the groupings found.</li>
</ul>

<p>Besides, there are several other reasons for adopt clustering analysis. Indeed, clustering has been successfully used in different  fields, including bioinformatics, image processing, and information retrieval.</p>

<h1 id="clustering-methods">Clustering methods</h1>

<p>Many clustering methods were designed based on different approaches such as partitional, hierarchical, probabilistic, and density-based. This post is about partitional methods.</p>

<p>Partitional clustering methods try to organize data into k clusters (where k is an
input parameter), by optimizing a certain objective function that captures a local and global structure of grouping. Most of the partitioning methods start with an initial assignment and then use an iterative relocation procedure which moves data points from one cluster to another to optimize the objective function.</p>

<p>On the other hand, partitional clustering methods often generate partitions (a division of the data set into mutually non-overlapping groups) where each data point belongs to one and only one cluster. These methods are the most commonly used because of their simplicity and their competitive computational complexity. One of the most popular and simple of these clustering algorithms is <strong>k-means</strong></p>

<h1 id="k-means-a-centroid-based-clustering-method">K-Means: A Centroid-based clustering method</h1>

<p>The k-means is the most fundamental partitional clustering method which is based on the idea that a center can represent a cluster.</p>

<p>A centroid is the most representative point within the cluster. Usually, this is the mean of the values of the points of data in the cluster. Given an initial clustering, centroid-based methods find the centroids of the clusters, reassign the data points to new clusters
defined by proximity to the centroids, and then repeat the procedure. The similarity between two clusters is the similarity between their respective centroids.</p>

<blockquote>
  <p>The most obvious measure of the similarity (or dissimilarity) between two samples is the distance between them. K-means is typically used with the Euclidean metric for computing the distance between points and cluster centers</p>
</blockquote>

<p>You must keep in mind that most obvious measure of the similarity (or dissimilarity) between two samples is the distance between them.
Thus, the key choices to be made, aside from an initial clustering, are the distance metric to be used, the centroids to be used, and how many iterations of the procedure to use.</p>

<h2 id="understanding-k-means">Understanding K-means</h2>

<p>Intuitively, the K-means clustering approach is the following. The analyst picks the number of clusters K and makes initial guesses about the cluster centers. The procedure starts at those K centers, and each center absorbs nearby points, based on distance (e.g Euclidean). Then, based on the absorbed cases, new cluster centers, usually the mean, are found. The procedure is then repeated: The new centers are allowed to absorb nearby points based on a norm, new centers are found, and so on. An example is exhibited in the video below:</p>

<div class="video-container">
	<center><iframe width="500" height="315" src="https://www.youtube.com/embed/Q7FMcIYm4aI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></center>
</div>

<p>Mathematically, we can represent K-means in this way:</p>

<p>Let $X = {x_j}, i = 1,‚Ä¶,n$ be the set of $n$ d-dimensional points to be clustered into a set of $K$ clusters, $C={c_k, k=1,‚Ä¶,K}$. K-means algorithm finds a partition such that the squared error between the empirical mean of a cluster and the points in the cluster is minimized. Let $\mu_k$ be the mean of cluster $c_k$ (centroid). The squared error between $\mu_k$ and the points in cluster $c_k$ is defined as</p>

<script type="math/tex; mode=display">J(c_k) = \sum_{x_i \in c_k}||x_i-\mu_k||^2</script>

<p>The goal of K-means is to minimize the sum of the squared error over all K clusters,</p>

<script type="math/tex; mode=display">J(c_k) = \sum_{k=1}^K \sum_{x_i \in c_k}||x_i-\mu_k||^2</script>

<p>The K-means algorithm requires three user-specified parameters:</p>

<ul>
  <li>Number of clusters K (the most critical choice)</li>
  <li>Cluster initialization</li>
  <li>Distance metric. K-means is typically used with the Euclidean metric for computing the distance between points and cluster centers.</li>
</ul>

<h2 id="number-of-clusters-of-k-means">Number of clusters of K-means</h2>

<p>Automatically determining the number of clusters has been one of the most difficult problems in data clustering. Most methods for automatically determining the number of clusters cast it into the problem of model selection. Usually, clustering algorithms are run with different values of K; the best value of K is then chosen based on a predefined criterion.</p>

<p>One method to validate the number of clusters is the elbow method. The idea of the elbow method is to run k-means clustering on the dataset for a range of values of k and for each value of k calculate the sum of squared errors (SSE). Furthermore, we plot a line chart of the SSE for each value of k. If the line chart looks like an arm, then the ‚Äúelbow‚Äù on the arm is the value of k that is the best. On the figure below, the most probable value for the initial number of clusters is 5 as the ‚Äúelbow‚Äù is in 4 and 5 intervals.</p>

<p class="image"><img src="/assets/img/clustering-analysis-an-introduction-elbow.png" alt="Clustring Analysis - An Introduction" /></p>

<h1 id="pratical-example-in-python-code">Pratical example in Python code</h1>

<p>For a practical example in Python code, check the video below (skip for 17:20 min). The video is available in portuguese for now.</p>

<div class="video-container">
	<center><iframe width="560" height="315" src="https://www.youtube.com/embed/xBIs5_ic5hU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></center>
</div>

<h2 id="references">References</h2>

<ol>
  <li>
    <p><a href="https://amzn.to/2Zo4t2k">Statistical Pattern Recognition - Andrew R. Webb</a></p>
  </li>
  <li>
    <p><a href="https://amzn.to/32dtcZ5">Pattern Classification - Richard O. Duda</a></p>
  </li>
</ol>
:ET